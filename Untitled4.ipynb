{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiteshsantwani/Machine_Learning_Project_3/blob/master/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXypChluOp5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "from scipy.io import loadmat\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from pylab import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxAIHGmlO6pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess():\n",
        "    \"\"\"\n",
        "     Input:\n",
        "     Although this function doesn't have any input, you are required to load\n",
        "     the MNIST data set from file 'mnist_all.mat'.\n",
        "\n",
        "     Output:\n",
        "     train_data: matrix of training set. Each row of train_data contains\n",
        "       feature vector of a image\n",
        "     train_label: vector of label corresponding to each image in the training\n",
        "       set\n",
        "     validation_data: matrix of training set. Each row of validation_data\n",
        "       contains feature vector of a image\n",
        "     validation_label: vector of label corresponding to each image in the\n",
        "       training set\n",
        "     test_data: matrix of training set. Each row of test_data contains\n",
        "       feature vector of a image\n",
        "     test_label: vector of label corresponding to each image in the testing\n",
        "       set\n",
        "    \"\"\"\n",
        "\n",
        "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
        "\n",
        "    n_feature = mat.get(\"train1\").shape[1]\n",
        "    n_sample = 0\n",
        "    for i in range(10):\n",
        "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
        "    n_validation = 1000\n",
        "    n_train = n_sample - 10 * n_validation\n",
        "\n",
        "    # Construct validation data\n",
        "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
        "    for i in range(10):\n",
        "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
        "\n",
        "    # Construct validation label\n",
        "    validation_label = np.ones((10 * n_validation, 1))\n",
        "    for i in range(10):\n",
        "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
        "\n",
        "    # Construct training data and label\n",
        "    train_data = np.zeros((n_train, n_feature))\n",
        "    train_label = np.zeros((n_train, 1))\n",
        "    temp = 0\n",
        "    for i in range(10):\n",
        "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
        "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
        "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
        "        temp = temp + size_i - n_validation\n",
        "\n",
        "    # Construct test data and label\n",
        "    n_test = 0\n",
        "    for i in range(10):\n",
        "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
        "    test_data = np.zeros((n_test, n_feature))\n",
        "    test_label = np.zeros((n_test, 1))\n",
        "    temp = 0\n",
        "    for i in range(10):\n",
        "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
        "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
        "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
        "        temp = temp + size_i\n",
        "\n",
        "    # Delete features which don't provide any useful information for classifiers\n",
        "    sigma = np.std(train_data, axis=0)\n",
        "    index = np.array([])\n",
        "    for i in range(n_feature):\n",
        "        if (sigma[i] > 0.001):\n",
        "            index = np.append(index, [i])\n",
        "    train_data = train_data[:, index.astype(int)]\n",
        "    validation_data = validation_data[:, index.astype(int)]\n",
        "    test_data = test_data[:, index.astype(int)]\n",
        "\n",
        "    # Scale data to 0 and 1\n",
        "    train_data /= 255.0\n",
        "    validation_data /= 255.0\n",
        "    test_data /= 255.0\n",
        "\n",
        "    return train_data, train_label, validation_data, validation_label, test_data, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnkeYGYePARl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkDXEQo4PDIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def blrObjFunction(initialWeights, *args):\n",
        "    \"\"\"\n",
        "    blrObjFunction computes 2-class Logistic Regression error function and\n",
        "    its gradient.\n",
        "\n",
        "    Input:\n",
        "        initialWeights: the weight vector (w_k) of size (D + 1) x 1\n",
        "        train_data: the data matrix of size N x D\n",
        "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
        "\n",
        "    Output:\n",
        "        error: the scalar value of error function of 2-class logistic regression\n",
        "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
        "                    error function\n",
        "    \"\"\"\n",
        "    train_data, labeli = args\n",
        "    n_data = train_data.shape[0]\n",
        "\n",
        "    ##################\n",
        "    # YOUR CODE HERE #\n",
        "    ##################\n",
        "    # HINT: Do not forget to add the bias term to your input data\n",
        "\n",
        "    # add bias terms\n",
        "    # compute theta\n",
        "    # compute error\n",
        "    # compute gradient error\n",
        "\n",
        "    bt = np.ones((n_data, 1))\n",
        "    new_train_data = np.hstack((bt, train_data))\n",
        "    z = np.dot(new_train_data, initialWeights)\n",
        "    theta = sigmoid(z)\n",
        "    theta = theta.reshape(theta.shape[0], 1)\n",
        "\n",
        "    op = labeli\n",
        "    # no need to compute likelihood.\n",
        "    a = (np.sum((op * np.log(theta)) + (1 - op) * np.log(1 - theta)))\n",
        "    error = -(1 / n_data) * a\n",
        "    b = (np.sum(((theta - op) * new_train_data), axis=0))\n",
        "    bTranpose = b.T\n",
        "    error_grad = (1 / n_data) * bTranpose\n",
        "\n",
        "    return error, error_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC34zD52PInQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def blrPredict(W, data):\n",
        "    \"\"\"\n",
        "     blrObjFunction predicts the label of data given the data and parameter W\n",
        "     of Logistic Regression\n",
        "\n",
        "     Input:\n",
        "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
        "         vector of a Logistic Regression classifier.\n",
        "         X: the data matrix of size N x D\n",
        "\n",
        "     Output:\n",
        "         label: vector of size N x 1 representing the predicted label of\n",
        "         corresponding feature vector given in data matrix\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ##################\n",
        "    # YOUR CODE HERE #\n",
        "    ##################\n",
        "    # HINT: Do not forget to add the bias term to your input data\n",
        "\n",
        "    bt = np.ones((data.shape[0], 1))\n",
        "    input_data = np.hstack((bt, data))\n",
        "    prediction = sigmoid(np.dot(input_data, W))  # the max probability in each row.\n",
        "    label_maxindex = np.argmax(prediction, axis=1)  # N\n",
        "    label = np.reshape(label_maxindex, (data.shape[0], 1))\n",
        "\n",
        "    return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxdGDo3yPNCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlrObjFunction(params, *args):\n",
        "    \"\"\"\n",
        "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
        "    its gradient.\n",
        "\n",
        "    Input:\n",
        "        initialWeights_b: the weight vector of size (D + 1) x 10\n",
        "        train_data: the data matrix of size N x D\n",
        "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
        "                representing the label of corresponding feature vector\n",
        "\n",
        "    Output:\n",
        "        error: the scalar value of error function of multi-class logistic regression\n",
        "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
        "                    error function\n",
        "    \"\"\"\n",
        "    train_data, labeli = args\n",
        "\n",
        "    n_data = train_data.shape[0]\n",
        "    n_feature = train_data.shape[1]\n",
        "    error_grad = np.zeros((n_feature + 1, n_class))\n",
        "\n",
        "    ##################\n",
        "    # YOUR CODE HERE #\n",
        "    ##################\n",
        "    # HINT: Do not forget to add the bias term to your input data\n",
        "    \n",
        "    # add bias item for input data\n",
        "    # compute weightV\n",
        "    # compute posterior probabilities\n",
        "    # compute error\n",
        "    # compute total error of each category\n",
        "    # compute gradient error\n",
        "    \n",
        "    bt = np.ones((n_data, 1))\n",
        "    new_train_data = np.hstack((bt, train_data))\n",
        "    weightV = params.reshape((n_feature + 1, n_class))\n",
        "    vector = np.dot(new_train_data, weightV)\n",
        "    a = np.sum(np.exp(vector), axis=1)\n",
        "    posteriorProbs = np.exp(vector) / np.reshape(a, (n_data, 1))\n",
        "    error = -np.sum(labeli * np.log(posteriorProbs))\n",
        "    \n",
        "    train_error_multiclass = []\n",
        "    for i in range(0, n_class):\n",
        "        error_multiclass = -np.sum(labeli[:, i] * np.log(posteriorProbs[:, i]))\n",
        "        train_error_multiclass.append(error_multiclass)\n",
        "    \n",
        "    for j in range(0, n_class):\n",
        "        error_grad[:, j] = np.sum((posteriorProbs[:, j] - np.transpose(labeli[:, j])) * np.transpose(new_train_data), axis=1)\n",
        "        \n",
        "    error_grad = error_grad.flatten()\n",
        "\n",
        "    return error, error_grad, train_error_multiclass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkBrNn8IoCuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlrPredict(W, data):\n",
        "    \"\"\"\n",
        "     mlrObjFunction predicts the label of data given the data and parameter W\n",
        "     of Logistic Regression\n",
        "\n",
        "     Input:\n",
        "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
        "         vector of a Logistic Regression classifier.\n",
        "         X: the data matrix of size N x D\n",
        "\n",
        "     Output:\n",
        "         label: vector of size N x 1 representing the predicted label of\n",
        "         corresponding feature vector given in data matrix\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ##################\n",
        "    # YOUR CODE HERE #\n",
        "    ##################\n",
        "    # HINT: Do not forget to add the bias term to your input data\n",
        "    \n",
        "    # add bias item for input data\n",
        "    # compute predicted label\n",
        "    \n",
        "    bt = np.ones((data.shape[0], 1))\n",
        "    input_data = np.hstack((bt, data))\n",
        "    z = np.dot(input_data, W)\n",
        "    prediction = np.exp(z) / np.reshape(np.sum(np.exp(z), axis=1),\n",
        "                                  (data.shape[0], 1))\n",
        "    label_maxindex = np.argmax(prediction, axis=1)\n",
        "    \n",
        "    label = np.reshape(label_maxindex, (data.shape[0], 1))\n",
        "\n",
        "    return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XzgTJU5qYNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3b057a38-9977-4f8d-95b9-a2f3cb66439e"
      },
      "source": [
        "\"\"\"\n",
        "Script for Logistic Regression\n",
        "\"\"\"\n",
        "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
        "\n",
        "n_class = 10  # number of classes\n",
        "n_train = train_data.shape[0]  # number of classes\n",
        "n_feature = train_data.shape[1]  # number of features\n",
        "\n",
        "######compute total error of training data for each category\n",
        "Y = np.zeros((n_train, n_class))\n",
        "for i in range(n_class):\n",
        "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
        "\n",
        "# Logistic Regression with Gradient Descent\n",
        "W = np.zeros((n_feature + 1, n_class))\n",
        "initialWeights = np.zeros((n_feature + 1, 1))\n",
        "opts = {'maxiter': 100}\n",
        "for i in range(n_class):\n",
        "    labeli = Y[:, i].reshape(n_train, 1)\n",
        "    args = (train_data, labeli)\n",
        "    nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
        "    W[:, i] = nn_params.x.reshape((n_feature + 1,))\n",
        "    # compute total error of training data with respect to each category\n",
        "    error, error_grad = blrObjFunction(W[:, i], *args)\n",
        "    print('\\n train_error_blr: ', error)\n",
        "print('-' * 100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_error_blr:  0.020215767855492867\n",
            "\n",
            " train_error_blr:  0.0211976519999382\n",
            "\n",
            " train_error_blr:  0.06227975067901295\n",
            "\n",
            " train_error_blr:  0.07509309541163409\n",
            "\n",
            " train_error_blr:  0.044561570321657175\n",
            "\n",
            " train_error_blr:  0.08235599662068284\n",
            "\n",
            " train_error_blr:  0.03444916331868657\n",
            "\n",
            " train_error_blr:  0.04379773082595095\n",
            "\n",
            " train_error_blr:  0.11004494540269226\n",
            "\n",
            " train_error_blr:  0.09731074374404708\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtbdxplrALO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "e5344e45-83b1-4d1a-8843-29a6e578abe0"
      },
      "source": [
        "# FOR EXTRA CREDIT ONLY\n",
        "n_class = 10 # number of classes\n",
        "n_train = train_data.shape[0] # number of classes\n",
        "n_feature = train_data.shape[1] # number of features\n",
        "# compute total error of training data for each category\n",
        "Y = np.zeros((n_train, n_class))\n",
        "for i in range(n_class):\n",
        "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
        "\n",
        "W_b = np.zeros((n_feature + 1, n_class))\n",
        "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
        "opts_b = {'maxiter': 100}\n",
        "\n",
        "args_b = (train_data, Y)\n",
        "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
        "W_b = nn_params.x.reshape((n_feature + 1, n_class))\n",
        "\n",
        "error_mlr, error_grad_mlr, train_error_mlr = mlrObjFunction(W_b, *args_b)\n",
        "print('\\n train_error_mlr: ', train_error_mlr)\n",
        "\n",
        "# Find the accuracy on Training Dataset\n",
        "predicted_label = blrPredict(W, train_data)\n",
        "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Validation Dataset\n",
        "predicted_label = blrPredict(W, validation_data)\n",
        "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Testing Dataset\n",
        "predicted_label = blrPredict(W, test_data)\n",
        "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Training Dataset\n",
        "predicted_label = mlrPredict(W_b, train_data)\n",
        "print('\\n Training set Accuracy Multi Class Logistic Regression:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Validation Dataset\n",
        "predicted_label = mlrPredict(W_b, validation_data)\n",
        "print('\\n Validation set Accuracy Multi Class Logistic Regression:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Testing Dataset\n",
        "predicted_label = mlrPredict(W_b, test_data)\n",
        "print('\\n Testing set Accuracy Multi Class Logistic Regression:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')\n",
        "\n",
        "n_class = 10 # number of features\n",
        "n_test = test_data.shape[0] # number of features\n",
        "n_feature = test_data.shape[1] # number of features\n",
        "\n",
        "######compute total error of training data for each category\n",
        "Y = np.zeros((n_test, n_class))\n",
        "for i in range(n_class):\n",
        "    Y[:, i] = (test_label == i).astype(int).ravel()\n",
        "\n",
        "# Logistic Regression with Gradient Descent\n",
        "initialWeights = np.zeros((n_feature + 1, 1))\n",
        "opts = {'maxiter': 100}\n",
        "for i in range(n_class):\n",
        "    labeli_test = Y[:, i].reshape(n_test, 1)\n",
        "    args_test = (test_data, labeli_test)\n",
        "    #compute total error of training data with respect to each category\n",
        "    error_test, error_grad = blrObjFunction(W[:, i], *args_test)\n",
        "    print('\\n test_error_blr: ', error_test)\n",
        "print('-'*100)\n",
        "\n",
        "# FOR EXTRA CREDIT ONLY\n",
        "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
        "opts_b = {'maxiter': 100}\n",
        "Y = np.zeros((n_test, n_class))\n",
        "for i in range(n_class):\n",
        "    Y[:, i] = (test_label == i).astype(int).ravel()\n",
        "\n",
        "args_b = (test_data, Y)\n",
        "error_mlr, error_grad_mlr, test_error_mlr = mlrObjFunction(W_b, *args_b)\n",
        "print('\\n Test Error Multi Class Logistic Regression: ', test_error_mlr)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_error_mlr:  [572.4710097252752, 676.3144528556601, 1642.0947917811059, 1635.765711131068, 1136.1128713850449, 1673.3907850970759, 639.812792779549, 1124.4654722784635, 1680.0155047471437, 1588.445440484902]\n",
            "\n",
            " Training set Accuracy:92.716%\n",
            "\n",
            " Validation set Accuracy:91.44%\n",
            "\n",
            " Testing set Accuracy:92.03%\n",
            "\n",
            " Training set Accuracy Multi Class Logistic Regression:93.156%\n",
            "\n",
            " Validation set Accuracy Multi Class Logistic Regression:92.54%\n",
            "\n",
            " Testing set Accuracy Multi Class Logistic Regression:92.54%\n",
            "\n",
            " test_error_blr:  0.026662412084946744\n",
            "\n",
            " test_error_blr:  0.02276963233245754\n",
            "\n",
            " test_error_blr:  0.0733170669535048\n",
            "\n",
            " test_error_blr:  0.07176029967394705\n",
            "\n",
            " test_error_blr:  0.05087249498551718\n",
            "\n",
            " test_error_blr:  0.08405494482456222\n",
            "\n",
            " test_error_blr:  0.037752252110886626\n",
            "\n",
            " test_error_blr:  0.05410566892198969\n",
            "\n",
            " test_error_blr:  0.11264025225124333\n",
            "\n",
            " test_error_blr:  0.10232484621252973\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            " Test Error Multi Class Logistic Regression:  [78.77808357223685, 128.8573131768607, 399.23876317629384, 279.3712089382686, 225.94194674387685, 365.09178862384636, 166.5156460237974, 304.1839862770219, 379.17034547091794, 339.08028017632773]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQgmtKBVrdfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1550
        },
        "outputId": "11aefa7e-196d-4770-a6c4-fe9876f80865"
      },
      "source": [
        "print('\\n\\n--------------SVM-------------------\\n\\n')\n",
        "\n",
        "# choose randomly 10000 rows of train, test and validation dataset\n",
        "\n",
        "index = np.random.choice(train_data.shape[0], int(0.2*train_data.shape[0]), replace=False)\n",
        "trainX_data = train_data[index]\n",
        "trainY_data = train_label[index]\n",
        "\n",
        "index = np.random.choice(validation_data.shape[0], int(0.2*validation_data.shape[0]), replace=False)\n",
        "validationX_Data = validation_data[index]\n",
        "validationY_Data = validation_label[index]\n",
        "\n",
        "index = np.random.choice(test_data.shape[0], int(0.2*test_data.shape[0]), replace=False)\n",
        "testX_data = test_data[index]\n",
        "testY_data = test_label[index]\n",
        "\n",
        "# linear kernel\n",
        "# fitting SVM into training, validation, test data\n",
        "\n",
        "# training data\n",
        "classifierLinearKernel = SVC(kernel='linear', random_state = 250)\n",
        "classifierLinearKernel.fit(trainX_data, trainY_data)\n",
        "\n",
        "y_pred = classifierLinearKernel.predict(trainX_data)\n",
        "acc = accuracy_score(y_pred, trainY_data)*100\n",
        "print('\\n train_accuracy(SVM using linear kernel): ' + str(acc) + '%')\n",
        "\n",
        "# validation data\n",
        "y_pred = classifierLinearKernel.predict(validationX_Data)\n",
        "acc = accuracy_score(y_pred, validationY_Data)*100\n",
        "print(' validation_accuracy(SVM using linear kernel): ' + str(acc) + '%')\n",
        "\n",
        "# test data\n",
        "y_pred = classifierLinearKernel.predict(testX_data)\n",
        "acc = accuracy_score(y_pred, testY_data)*100\n",
        "print(' test_accuracy(SVM using linear kernel): ' + str(acc) + '%')\n",
        "\n",
        "# rbf with gamma = 1\n",
        "# fitting SVM into training, validation, test data\n",
        "\n",
        "# training data\n",
        "classifierRBF = SVC(kernel='rbf', gamma=1, random_state = 250)\n",
        "classifierRBF.fit(trainX_data, trainY_data)\n",
        "\n",
        "y_pred = classifierRBF.predict(trainX_data)\n",
        "acc = accuracy_score(y_pred, trainY_data)*100\n",
        "print('\\n train_accuracy(SVM using rbf and gamma=1 kernel): ' + str(acc) + '%')\n",
        "\n",
        "# validation data\n",
        "y_pred = classifierRBF.predict(validationX_Data)\n",
        "acc = accuracy_score(y_pred, validationY_Data)*100\n",
        "print(' validation_accuracy(SVM using rbf and gamma=1 kernel): ' + str(acc) + '%')\n",
        "\n",
        "# test data\n",
        "y_pred = classifierRBF.predict(testX_data)\n",
        "acc = accuracy_score(y_pred, testY_data)*100\n",
        "print(' test_accuracy(SVM using rbf and gamma=1 kernel): ' + str(acc) + '%')\n",
        "\n",
        "# rbf with gamma=default\n",
        "# fitting SVM into training, validation, test data\n",
        "\n",
        "# training data\n",
        "classifierRBFauto = SVC(kernel='rbf', gamma='auto', random_state = 250)\n",
        "classifierRBFauto.fit(trainX_data, trainY_data)\n",
        "\n",
        "y_pred = classifierRBFauto.predict(trainX_data)\n",
        "acc = accuracy_score(y_pred, trainY_data)*100\n",
        "print('\\n train_accuracy(SVM using rbf and gamma=auto kernel): ' + str(acc) + '%')\n",
        "\n",
        "# validation data\n",
        "y_pred = classifierRBFauto.predict(validationX_Data)\n",
        "acc = accuracy_score(y_pred, validationY_Data)*100\n",
        "print(' validation_accuracy(SVM using rbf and gamma=auto kernel): ' + str(acc) + '%')\n",
        "\n",
        "# test data\n",
        "y_pred = classifierRBFauto.predict(testX_data)\n",
        "acc = accuracy_score(y_pred, testY_data)*100\n",
        "print(' test_accuracy(SVM using rbf and gamma=auto kernel): ' + str(acc) + '%')\n",
        "\n",
        "# rbf with gamma=default, C = 1,10,20,30,...,100 and plot graph of accuracy\n",
        "C = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "for i in C:\n",
        "    \n",
        "    # training data\n",
        "    classifier = SVC(kernel='rbf', gamma='auto', C=i, random_state = 250)\n",
        "    classifier.fit(trainX_data, trainY_data)\n",
        "    \n",
        "    y_pred = classifier.predict(trainX_data)\n",
        "    acc_c1 = accuracy_score(y_pred, trainY_data)*100\n",
        "    print('\\n train_accuracy(SVM using rbf, gamma=auto and C=' + str(i) + ' kernel): ' + str(acc_c1) + '%')\n",
        "\n",
        "    # validation data\n",
        "    y_pred = classifier.predict(validationX_Data)\n",
        "    acc_c2 = accuracy_score(y_pred, validationY_Data)*100\n",
        "    print(' validation_accuracy(SVM using rbf, gamma=auto and C=' + str(i) + ' kernel): ' + str(acc_c2) + '%')\n",
        "\n",
        "    # test data\n",
        "    y_pred = classifier.predict(testX_data)\n",
        "    acc_c3 = accuracy_score(y_pred, testY_data)*100\n",
        "    print(' test_accuracy(SVM using rbf, gamma=auto and C=' + str(i) + ' kernel): ' + str(acc_c3) + '%')\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--------------SVM-------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using linear kernel): 99.67%\n",
            " validation_accuracy(SVM using linear kernel): 92.75%\n",
            " test_accuracy(SVM using linear kernel): 91.60000000000001%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf and gamma=1 kernel): 100.0%\n",
            " validation_accuracy(SVM using rbf and gamma=1 kernel): 13.5%\n",
            " test_accuracy(SVM using rbf and gamma=1 kernel): 16.35%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf and gamma=auto kernel): 92.67999999999999%\n",
            " validation_accuracy(SVM using rbf and gamma=auto kernel): 93.2%\n",
            " test_accuracy(SVM using rbf and gamma=auto kernel): 91.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=1 kernel): 92.67999999999999%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=1 kernel): 93.2%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=1 kernel): 91.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=10 kernel): 96.45%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=10 kernel): 94.85%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=10 kernel): 94.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=20 kernel): 97.64%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=20 kernel): 95.3%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=20 kernel): 94.3%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=30 kernel): 98.33%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=30 kernel): 95.39999999999999%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=30 kernel): 94.6%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=40 kernel): 98.87%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=40 kernel): 95.5%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=40 kernel): 94.6%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=50 kernel): 99.13%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=50 kernel): 95.7%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=50 kernel): 94.55%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=60 kernel): 99.4%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=60 kernel): 95.89999999999999%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=60 kernel): 94.5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=70 kernel): 99.56%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=70 kernel): 95.85000000000001%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=70 kernel): 94.5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=80 kernel): 99.69%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=80 kernel): 95.7%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=80 kernel): 94.55%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=90 kernel): 99.8%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=90 kernel): 95.6%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=90 kernel): 94.8%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train_accuracy(SVM using rbf, gamma=auto and C=100 kernel): 99.87%\n",
            " validation_accuracy(SVM using rbf, gamma=auto and C=100 kernel): 95.7%\n",
            " test_accuracy(SVM using rbf, gamma=auto and C=100 kernel): 94.65%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}